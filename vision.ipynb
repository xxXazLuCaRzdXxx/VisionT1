{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install core ML libraries\n!pip install --quiet transformers evaluate scikit-learn av\n\n# Install PyTorchVideo straight from GitHub (avoids PyPI build issues)\n!pip install --quiet git+https://github.com/facebookresearch/pytorchvideo.git\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:18:11.280781Z","iopub.execute_input":"2025-05-18T17:18:11.281034Z","iopub.status.idle":"2025-05-18T17:18:28.571706Z","shell.execute_reply.started":"2025-05-18T17:18:11.281012Z","shell.execute_reply":"2025-05-18T17:18:28.570740Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, tarfile\nfrom pathlib import Path\nfrom huggingface_hub import hf_hub_download\n\n# Download UCF101 subset (pre-packaged for easy loading)\nrepo_id = \"sayakpaul/ucf101-subset\"\nfile_name = \"UCF101_subset.tar.gz\"\ntar_path = hf_hub_download(repo_id=repo_id, filename=file_name, repo_type=\"dataset\")\ndata_root = Path(\"data/ucf101\")\nwith tarfile.open(tar_path) as tar:\n    tar.extractall(path=data_root)\n\n# The extracted folder should contain 'train' and 'test' subdirectories\ntrain_dir = data_root / \"UCF101_subset\" / \"train\"\ntest_dir  = data_root / \"UCF101_subset\" / \"test\"\n\n# Define which classes we consider as \"cyberbullying\"\ntarget_classes = {\n    \"BaseballPitch\",# hitting a punching bag (proxy for boxing)\n    \"BenchPress\",     # hitting a speed bag\n    \"BasketballDunk\",              # person punching something\n    \"Basketball\"       # wrestling\n}\n\n# Verify these classes exist in the train directory\nclasses = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\nprint(\"Classes in dataset (sample):\", classes[:10])\nfor cls in target_classes:\n    if cls not in classes:\n        raise ValueError(f\"Class {cls} not found in dataset classes\")\n\n# Map class names to indices (assuming UCF101 loader uses sorted order)\nclass_to_idx = {c: i for i, c in enumerate(classes)}\ntarget_indices = {class_to_idx[c] for c in target_classes}\n\nprint(\"Binary label mapping: {} classes labeled as bullying\".format(len(target_indices)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:18:32.378821Z","iopub.execute_input":"2025-05-18T17:18:32.379047Z","iopub.status.idle":"2025-05-18T17:18:38.694031Z","shell.execute_reply.started":"2025-05-18T17:18:32.379025Z","shell.execute_reply":"2025-05-18T17:18:38.693240Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"UCF101_subset.tar.gz:   0%|          | 0.00/171M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70033d28783d43bf949ba6f07f1aa152"}},"metadata":{}},{"name":"stdout","text":"Classes in dataset (sample): ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress']\nBinary label mapping: 4 classes labeled as bullying\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n\n# Load pretrained VideoMAE model and image processor\nmodel_ckpt = \"MCG-NJU/videomae-base\"\nimage_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\nlabel2id = {\"not_bullying\": 0, \"cyberbullying\": 1}\nid2label = {v: k for k, v in label2id.items()}\nmodel = VideoMAEForVideoClassification.from_pretrained(\n    model_ckpt,\n    label2id=label2id, id2label=id2label,\n    ignore_mismatched_sizes=True\n)\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:18:42.674435Z","iopub.execute_input":"2025-05-18T17:18:42.674960Z","iopub.status.idle":"2025-05-18T17:19:14.418497Z","shell.execute_reply.started":"2025-05-18T17:18:42.674936Z","shell.execute_reply":"2025-05-18T17:19:14.417828Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 17:18:54.542083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747588734.777787      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747588734.855229      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04eecc33b2324821af38fe6d9fbb8c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2dc403683d41cd8e4549e5f9ff9c79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad02b5caba894b2382986573ceb458c0"}},"metadata":{}},{"name":"stderr","text":"Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"VideoMAEForVideoClassification(\n  (videomae): VideoMAEModel(\n    (embeddings): VideoMAEEmbeddings(\n      (patch_embeddings): VideoMAEPatchEmbeddings(\n        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n      )\n    )\n    (encoder): VideoMAEEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VideoMAELayer(\n          (attention): VideoMAEAttention(\n            (attention): VideoMAESelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (output): VideoMAESelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): VideoMAEIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VideoMAEOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from pytorchvideo.transforms import (\n    ApplyTransformToKey, Normalize, RandomShortSideScale,\n    UniformTemporalSubsample\n)\nfrom torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, Resize\n\nmean = image_processor.image_mean\nstd = image_processor.image_std\nif \"shortest_edge\" in image_processor.size:\n    height = width = image_processor.size[\"shortest_edge\"]\nelse:\n    height = image_processor.size[\"height\"]\n    width  = image_processor.size[\"width\"]\nresize_to = (height, width)\n\nnum_frames = model.config.num_frames  # typically 16\nsample_rate = 4\nfps = 30\nclip_duration = num_frames * sample_rate / fps\n\n# Training transforms: sampling + augmentation\ntrain_transform = Compose([\n    ApplyTransformToKey(\n        key=\"video\",\n        transform=Compose([\n            UniformTemporalSubsample(num_frames),\n            Lambda(lambda x: x/255.0),\n            Normalize(mean, std),\n            RandomShortSideScale(min_size=256, max_size=320),\n            RandomCrop(resize_to),\n            RandomHorizontalFlip(p=0.5),\n        ])\n    )\n])\n\n# Validation transforms: uniform sampling, no augmentation\nval_transform = Compose([\n    ApplyTransformToKey(\n        key=\"video\",\n        transform=Compose([\n            UniformTemporalSubsample(num_frames),\n            Lambda(lambda x: x/255.0),\n            Normalize(mean, std),\n            Resize(resize_to),\n        ])\n    )\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:19:44.810014Z","iopub.execute_input":"2025-05-18T17:19:44.810672Z","iopub.status.idle":"2025-05-18T17:19:44.825255Z","shell.execute_reply.started":"2025-05-18T17:19:44.810644Z","shell.execute_reply":"2025-05-18T17:19:44.824701Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom pytorchvideo.data.ucf101 import Ucf101\nfrom pytorchvideo.data.clip_sampling import RandomClipSampler, UniformClipSampler\nfrom pytorchvideo.transforms import ApplyTransformToKey, Normalize, UniformTemporalSubsample\nfrom torchvision.transforms import Compose, Lambda, RandomHorizontalFlip, CenterCrop, RandomResizedCrop\n\n\n# 2️⃣ Video-only transforms\nvideo_train_transform = Compose([\n    UniformTemporalSubsample(num_frames),\n    Lambda(lambda x: x / 255.0),\n    RandomResizedCrop((224, 224)),\n    RandomHorizontalFlip(p=0.5),\n    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n])\nvideo_val_transform = Compose([\n    UniformTemporalSubsample(num_frames),\n    Lambda(lambda x: x / 255.0),\n    CenterCrop((224, 224)),\n    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n])\ntrain_transform = ApplyTransformToKey(key=\"video\", transform=video_train_transform)\nval_transform   = ApplyTransformToKey(key=\"video\", transform=video_val_transform)\n\n# 3️⃣ Paths\ntrain_dir = \"/kaggle/working/data/ucf101/UCF101_subset/train\"\ntest_dir  = \"/kaggle/working/data/ucf101/UCF101_subset/test\"\n\n# 4️⃣ Base datasets\ntrain_base = Ucf101(data_path=train_dir,\n                    clip_sampler=RandomClipSampler(clip_duration),\n                    decode_audio=False,\n                    transform=train_transform)\nval_base   = Ucf101(data_path=test_dir,\n                    clip_sampler=UniformClipSampler(clip_duration),\n                    decode_audio=False,\n                    transform=val_transform)\n\n# 5️⃣ Iterable wrapper: convert int label → torch.tensor(0/1)\nclass BinaryUCF101(IterableDataset):\n    def __init__(self, base_ds, target_idxs):\n        super().__init__()\n        self.base = base_ds\n        self.target_idxs = target_idxs\n\n    def __iter__(self):\n        for sample in self.base:\n            orig_label = sample[\"label\"]          # this is already an int\n            sample[\"label\"] = torch.tensor(\n                1 if orig_label in self.target_idxs else 0\n            )\n            yield sample\n\n# 6️⃣ Instantiate binary datasets & dataloaders\ntrain_ds = BinaryUCF101(train_base, target_indices)\nval_ds   = BinaryUCF101(val_base,   target_indices)\n\ntrain_loader = DataLoader(train_ds, batch_size=4, num_workers=2)\nval_loader   = DataLoader(val_ds,   batch_size=4, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:34:31.891933Z","iopub.execute_input":"2025-05-18T17:34:31.892289Z","iopub.status.idle":"2025-05-18T17:34:31.905773Z","shell.execute_reply.started":"2025-05-18T17:34:31.892254Z","shell.execute_reply":"2025-05-18T17:34:31.905289Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"batch = next(iter(train_loader))\nprint(\"Video batch shape:\", batch[\"video\"].shape)   # expected (B, T, C, H, W)\nprint(\"Labels:\", batch[\"label\"], type(batch[\"label\"][0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:34:45.877298Z","iopub.execute_input":"2025-05-18T17:34:45.877589Z","iopub.status.idle":"2025-05-18T17:34:50.684134Z","shell.execute_reply.started":"2025-05-18T17:34:45.877570Z","shell.execute_reply":"2025-05-18T17:34:50.683361Z"}},"outputs":[{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>\nException ignored in: Traceback (most recent call last):\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\nTraceback (most recent call last):\n      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\nself._shutdown_workers()    \n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\nself._shutdown_workers()\n    if w.is_alive():  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n\n      if w.is_alive():   \n     ^ ^ ^^^^ ^ ^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ \n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n      assert self._parent_pid == os.getpid(), 'can only test a child process' \n             ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: ^can only test a child process^\n^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Video batch shape: torch.Size([4, 3, 16, 224, 224])\nLabels: tensor([0, 0, 1, 1]) <class 'torch.Tensor'>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.optim import AdamW\nfrom sklearn.metrics import f1_score, accuracy_score\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    model.train()\n    train_preds, train_labels = [], []\n    for batch in train_loader:\n        videos = batch[\"video\"].to(device)          # shape: (B, C, T, H, W)\n        labels = batch[\"label\"].to(device)          # shape: (B,)\n        optimizer.zero_grad()\n        outputs = model(pixel_values=videos, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        # Collect predictions\n        logits = outputs.logits.detach().cpu()\n        preds = torch.argmax(logits, dim=1).numpy()\n        train_preds.extend(preds.tolist())\n        train_labels.extend(labels.cpu().numpy().tolist())\n    # Compute training metrics\n    train_acc = accuracy_score(train_labels, train_preds)\n    train_f1  = f1_score(train_labels, train_preds)\n    \n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            videos = batch[\"video\"].to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = model(pixel_values=videos)\n            logits = outputs.logits.cpu()\n            preds = torch.argmax(logits, dim=1).numpy()\n            val_preds.extend(preds.tolist())\n            val_labels.extend(labels.cpu().numpy().tolist())\n    val_acc = accuracy_score(val_labels, val_preds)\n    val_f1  = f1_score(val_labels, val_preds)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n          f\"Train Acc={train_acc:.3f}, F1={train_f1:.3f}; \"\n          f\"Val Acc={val_acc:.3f}, F1={val_f1:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:34:58.971130Z","iopub.execute_input":"2025-05-18T17:34:58.971457Z","iopub.status.idle":"2025-05-18T17:35:03.763045Z","shell.execute_reply.started":"2025-05-18T17:34:58.971429Z","shell.execute_reply":"2025-05-18T17:35:03.761749Z"}},"outputs":[{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n     Exception ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>^\n^Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ \n   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n      assert self._parent_pid == os.getpid(), 'can only test a child process' \n  ^ ^^^  ^ ^^  ^^ ^ ^^ \n ^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^ ^ ^ ^  ^ ^ ^ ^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError^^: ^can only test a child process\n^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1693671862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# shape: (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/modeling_videomae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         outputs = self.videomae(\n\u001b[0m\u001b[1;32m   1079\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/modeling_videomae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/modeling_videomae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# create patch embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# add position embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/modeling_videomae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;34m\"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Make sure that the channel dimension of the pixel values match with the one set in the configuration."],"ename":"ValueError","evalue":"Make sure that the channel dimension of the pixel values match with the one set in the configuration.","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.optim import AdamW\nfrom sklearn.metrics import f1_score, accuracy_score\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    model.train()\n    train_preds, train_labels = [], []\n    for batch in train_loader:\n        # permute from (B, C, T, H, W) → (B, T, C, H, W)\n        videos = batch[\"video\"].permute(0, 2, 1, 3, 4).to(device)\n        labels = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        outputs = model(pixel_values=videos, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        # Collect predictions\n        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        train_preds.extend(preds.tolist())\n        train_labels.extend(labels.cpu().numpy().tolist())\n\n    train_acc = accuracy_score(train_labels, train_preds)\n    train_f1  = f1_score(train_labels, train_preds)\n\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            videos = batch[\"video\"].permute(0, 2, 1, 3, 4).to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = model(pixel_values=videos)\n            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            val_preds.extend(preds.tolist())\n            val_labels.extend(labels.cpu().numpy().tolist())\n\n    val_acc = accuracy_score(val_labels, val_preds)\n    val_f1  = f1_score(val_labels, val_preds)\n\n    print(\n        f\"Epoch {epoch+1}/{num_epochs}: \"\n        f\"Train Acc={train_acc:.3f}, F1={train_f1:.3f}; \"\n        f\"Val Acc={val_acc:.3f}, F1={val_f1:.3f}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:36:35.052763Z","iopub.execute_input":"2025-05-18T17:36:35.053558Z","iopub.status.idle":"2025-05-18T17:42:34.005810Z","shell.execute_reply.started":"2025-05-18T17:36:35.053526Z","shell.execute_reply":"2025-05-18T17:42:34.005028Z"}},"outputs":[{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\nException ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>self._shutdown_workers()\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n        if w.is_alive():self._shutdown_workers()\n \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive(): \n  ^ ^ ^ ^  ^ ^^^^^^^^^^^^^\n^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ \n   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n      assert self._parent_pid == os.getpid(), 'can only test a child process' \n           ^ ^ ^  ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: ^can only test a child process^\n\nAssertionErrorException ignored in: : can only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>\n\nException ignored in: Traceback (most recent call last):\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7954f56811c0>  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n\nTraceback (most recent call last):\n      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\nself._shutdown_workers()\n      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\nself._shutdown_workers()    \nif w.is_alive():  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n\n     if w.is_alive(): \n          ^^ ^ ^^^^^^^^^^^^^^^^^^^^\n^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n\n                    ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: \nAssertionErrorcan only test a child process\n: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3: Train Acc=0.637, F1=0.493; Val Acc=0.742, F1=0.630\nEpoch 2/3: Train Acc=0.793, F1=0.721; Val Acc=0.684, F1=0.675\nEpoch 3/3: Train Acc=0.917, F1=0.897; Val Acc=0.813, F1=0.603\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\n# 1. Create a test DataLoader exactly like train/val\ntest_base = Ucf101(\n    data_path=\"/kaggle/working/data/ucf101/UCF101_subset/test\",\n    clip_sampler=UniformClipSampler(clip_duration),\n    decode_audio=False,\n    transform=val_transform,        # same as validation\n)\ntest_ds = BinaryUCF101(test_base, target_indices)\ntest_loader = DataLoader(test_ds, batch_size=4, num_workers=2)\n\n# 2. Run inference on test set\nmodel.eval()\ntest_preds, test_labels = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        videos = batch[\"video\"].permute(0, 2, 1, 3, 4).to(device)\n        labels = batch[\"label\"].to(device)\n        logits = model(pixel_values=videos).logits\n        preds = logits.argmax(dim=-1).cpu().numpy()\n        test_preds.extend(preds.tolist())\n        test_labels.extend(labels.cpu().numpy().tolist())\n\n# 3. Compute metrics\ntest_acc = accuracy_score(test_labels, test_preds)\ntest_f1  = f1_score(test_labels, test_preds)\nprint(f\"Test  Acc={test_acc:.3f}  |  Test F1={test_f1:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:44:37.133403Z","iopub.execute_input":"2025-05-18T17:44:37.133748Z","iopub.status.idle":"2025-05-18T17:45:05.462412Z","shell.execute_reply.started":"2025-05-18T17:44:37.133720Z","shell.execute_reply":"2025-05-18T17:45:05.461536Z"}},"outputs":[{"name":"stdout","text":"Test  Acc=0.813  |  Test F1=0.603\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}